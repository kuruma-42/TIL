### **전송 지연(transmission delay)과 전파 지연(propagation delay)의 비교**

**전송 지연**

- **라우터가 패킷을 내보내는 데** 필요한 시간
- 패킷 길이와 링크 전송률의 함수 → 두 라우터 사이의 거리와는 관계가 없다.

**전파 지연**

- 비트가 **한 라우터에서 다음 라우터로 전파되는 데** 걸리는 시간
- 두 라우터 사이의 거리에 대한 함수 → 패킷 길이나 링크 전송률과는 관계가 없다.

### **패킷 교환 대 회선 교환**

**[ 패킷 교환 옹호자 ]**

- 주장
    1. 패킷 교환이 회선 교환보다 전송 용량의 공유에서 더 효율적이다.
    2. 패킷 교환이 회선 교환보다 더 간단하고 효율적이며, 구현 비용이 적다.
- 근거
    - 회선 교환에서 통신을 위해서는 자원이 항상 **각각의** 사용자에게 **예약**되어야만 한다.
    - 할당된 회선이 `비활용 기간(silent period)`에는 자원을 점유한 채로 놀게 되기 때문에 자원 이용률이 감소한다.
    - 즉, 회선 교환에서는 사용되지 않는 네트워크 자원(연결 경로상의 링크 주파수 대역이나 슬롯)은 **다른 진행 중인 연결이 대신해서 사용할 수 없기 때문에**패킷 교환이 더 효율적이다.

**[ 패킷 교환 반대자 ]**

- 주장 : 패킷 교환은 실시간 서비스에는 적당하지 않다.
- 근거 : 주로 큐잉 지연에서 발생하는 **종단 간의 지연** (불규칙적이고 예측할 수 없음)

과연 패킷 교환 반대자의 주장은 옳을까?

이를 확인해보기 위해서 간단한 예 두 가지를 살펴보자.

---

```
1. 사용자가 1 Mbps 링크를 공유한다고 가정하고, 각 사용자들은 활동 시간과 비활동 시간을 반복한다고 하자.
사용자는 전체 시간에서 10%만 활동하며 나머지 90% 시간에는 활동하지 않는다.

```

- 활동 시간 : 100 kbps의 일정 속도로 데이터를 생산할 때
- 비활동 시간 : 데이터를 생산하지 않을 때

✅ `회선 교환`의 경우, 100 kbps가 항상 각각의 사용자에게 예약되어야 한다.

TDM 회선 교환을 예시로, 초 프레임이 100 ms마다 10개 시간 슬롯으로 나뉜다고 한다면 각 사용자에게는 **한 프레임에 한 번의 시간 슬롯**이 할당된다.

따라서 회선 교환 링크는 **동시에 10명(= 1 Mbps / 100 kbps)만 지원할 수 있다.**

✅ `패킷 교환`의 경우, 한 특정 사용자가 활동을 하고 있을 확률은 10%이다.

만약 10명 이하의 동시 사용자가 있다면 그 확률은 99.96%, 데이터의 통합 도착률은 1 Mbps(링크의 출력률)보다 작거나 같다.

따라서 **10명 이상의 동시 사용자가 있다면 패킷의 통합 도착률이 링크의 출력 용량을 초과하기 때문에 출력 큐가 커지기 시작한다.**

(이 큐는 통합 입력률이 1 Mbps 이하로 떨어질 때까지 커질 것이고, 이후에는 큐 길이가 줄어들기 시작할 것)

10명 이상의 동시 사용자가 있을 확률은 0.04%로 굉장히 작으므로,

**패킷 교환은 거의 항상 회선 교환과 대등한 지연 성능을 가지면서도** 거의 3배 이상의 사용자 수를 허용한다.

---

```
2. 10명의 사용자가 있다고 가정하자. 1번과 동일하게, 사용자는 1 Mbps 링크를 공유한다.
한 사용자가 한번에 1,000비트 패킷을 1,000개 생성하고 다른 사용자는 패킷을 생성하지 않는다.

```

✅ `회선 교환`의 경우를 먼저 보자.

TDM 회선 교환을 예시로, 한 프레임은 10개 슬롯으로 구성되고 각 슬롯은 1,000비트로 구성되었다면

사용자는 데이터 전송을 위해 **한 프레임당 1개의 시간 슬롯만 사용할 수 있다.** 반면에 각 프레임에 남겨진 9개의 시간 슬롯은 쉬는 상태가 된다.

따라서 사용자의 데이터 **100만 비트를 모두 전송하려면 10초가 걸린다.**

✅ `패킷 교환`의 경우,

패킷을 생성하는 다른 사용자가 없기 때문에 다중화가 요구되지 않고, 사용자는 1 Mbps의 링크가 가득 찰 때까지 패킷을 계속 보낼 수 있다.

따라서 사용자의 데이터 100만 비트는 **1초 만에 모두 전송된다.**

---

앞의 두 가지 예에서 명확하게 볼 수 있듯, **패킷 교환이 회선 교환보다 성능이 우수하다.**

따라서 오늘날의 전기통신 네트워크의 추세는 패킷 교환으로 전환되고 있다.

링크 전송률을 공유하는 두 방식의 가장 큰 차이점은 아래와 같이 정리할 수 있다.

- 회선 교환 방식 : 요구에 관계없이 미리 전송 링크의 사용을 할당한다.
- 패킷 교환 방식 : **요구할 때만** 링크의 사용을 할당한다

### **캡슐화(encapsulation)**

> 💡 각 계층에서 패킷은 헤더 필드와 페이로드 필드(payload field)라는 두 가지 형태의 필드를 갖는다.
> 

페이로드(payload)는 일반적으로 그 계층 상위로부터의 패킷을 말한다.

**캡슐화 과정**

1. 송신 호스트에서 `애플리케이션 계층 메시지(application-layer message, 위 그림에서의 M)`는 트랜스포트 계층으로 보내진다.
2. 가장 간단한 경우, 트랜스포트 계층은 메시지에 수신 측 트랜스포트 계층에서 사용될 추가 정보인 `트랜스포트 계층 헤더 정보(Ht)`를 더한다.

> 트랜스포트 계층 세그먼트(transport-layer segment) = 애플리케이션 계층 메시지 + 트랜스포트 계층 헤더 정보
> 
- 트랜스포트 계층 세그먼트는 애플리케이션 계층 메시지를 **캡슐화**한다.
- 트랜스포트 계층 헤더 정보가 포함하는 내용은 다음과 같다.
    - 수신 측의 트랜스포트 계층이 그 메시지를 적절한 애플리케이션으로 보내도록 하는 정보들
    - 메시지의 비트들이 변경되었는지 아닌지를 수신자가 결정하게 하는 오류 검출 비트
1. 트랜스포트 계층은 세그먼트를 네트워크 계층으로 보낸다.
2. 네트워크 계층은 출발지와 목적지 종단 시스템 주소와 동일한 헤더 정보(Hn)를 추가하여 `네트워크 계층 데이터그램(network-layer datagram)`을 만든다.
3. 데이터그램은 링크 계층으로 전달된다.
4. 링크 계층도 자신의 헤더 정보를 추가하여 `링크 계층 프레임(link-layer frame)`을 만든다.

### TCP/UDP

**1️⃣ TCP 서비스**

TCP 전송 프로토콜은 다음 세 가지 서비스를 제공한다.

**연결 지향형 서비스**

**애플리케이션 계층 메시지를 전송하기 전에 TCP는 클라이언트와 서버가 서로 전송 제어 정보를 교환하게 한다.**

이 `핸드 셰이킹(handshaking) 과정`이 클라이언트와 서버에 패킷이 곧 도달할테니 준비하라고 알려주는 역할을 한다.

핸드셰이킹 단계를 지나면, TCP 연결이 두 프로세스의 소켓 사이에 존재한다고 말한다.

이 연결은 두 프로세스가 서로에게 동시에 메시지를 보낼 수 있기에 `전이중 연결`이라고 한다. (3장에서 더 자세히 다룬다.)

**신뢰적인 데이터 전송 서비스**

통신 프로세스는 **모든 데이터를 오류 없이 올바른 순서로 전달하기 위해** TCP에 의존한다.

TCP는 애플리케이션의 한 쪽이 바이트 스트림을 소켓으로 전달하면, 그 바이트 스트림이 손실되거나 중복되지 않게 수신 소켓으로 전달한다.

**혼잡 제어 방식**

네트워크가 혼잡 상태에 이르면 프로세스의 속도를 낮추는 방법 (또한, 3장에서 자세히 다룬다.)

**2️⃣ UDP 서비스**

UDP는 최소의 서비스 모델을 가진 간단한 전송 프로토콜이다.

UDP는 **비연결형**으로 핸드셰이킹 과정이 없고, **비신뢰적인 데이터 전송 서비스**를 제공하여 데이터가 전달되는 것을 보장하지 않는다.

UDP는 또한 **혼잡 제어 방식을 포함하지 않아** 프로세스의 속도 저하 없이 네트워크를 이용할 수 있다.

그러나 혼잡으로 인해 종단 간 처리율이 낮아져서 속도가 오히려 더 낮아질 수 있다

### HTTP

`HTTP`는 **메시지의 구조** 및 **클라이언트와 서버가 메시지를 어떻게 교환하는지**에 대해 정의하고 있다.

**비지속(non-persistent) 연결**

> 💡 클라이언트-서버 상호작용이 TCP 상에서 발생할 때 각 요구/응답 쌍이 분리된 TCP 연결을 통해 보내지는 것을 말한다.
> 

웹 페이지를 서버에서 클라이언트로 전송하는 단계를 살펴보자.

페이지가 **기본 HTML 파일과 10개의 이미지**로 구성되고, 이 11개의 객체가 같은 서버에 있다고 가정하자.

연결 수행 과정은 다음과 같다.

1. HTTP 클라이언트는 `HTTP 기본 포트 80`을 통해 서버로 `TCP 연결`을 시도한다. TCP 연결과 관련하여 클라이언트와 서버에 각각 소켓이 있게 된다.
2. HTTP 클라이언트는 설정된 TCP 연결 소켓을 통해 서버로 HTTP 요청 메시지를 보낸다. 이 요청에 객체 경로도 포함된다.
3. HTTP 서버는 TCP 연결 소켓을 통해 요청 메시지를 받는다. 저장 장치로부터 경로의 객체를 추출한다.
    
    HTTP 응답 메시지에 그 객체를 캡슐화 하여 소켓을 통해 클라이언트로 보낸다.
    
4. HTTP 서버는 TCP에게 연결을 끊으라고 한다. (그러나 실제로 클라이언트가 응답 메시지를 올바로 받을 때까지 끊지 않는다.)
5. HTTP 클라이언트가 응답 메시지를 받으면, TCP 연결이 중단된다. 메시지는 캡슐화된 객체가 HTML 파일인 것을 나타낸다.
    
    클라이언트는 응답 메시지로부터 파일을 추출하고 HTML 파일을 조사하여 10개의 JPEG 객체에 대한 참조를 찾는다.
    
6. **참조되는 JPEG 객체에 대해 1 ~ 4단계를 반복한다.**

브라우저는 웹 페이지를 수신하면서, 사용자에게 그 페이지를 보여준다. 다른 브라우저는 웹 페이지를 각기 다른 방식으로 해석하여 보여준다.

> HTTP는 통신 프로토콜만 정의할 뿐, 웹 페이지에 대한 관심은 없다.
> 

사용자는 앞의 단계를 동시에 받을지 순차적으로 받을지의 동시성 정도를 조절할 수 있도록 브라우저를 구성할 수 있다.

브라우저는 **여러 개의 TCP 연결을 설정하며 다중 연결상에서 웹 페이지의 각기 다른 원하는 부분을 요청**할 수 있다.

HTTP 1.0은 비지속 연결을 지원한다.

클라이언트가 HTML 파일을 요청하고 그 파일이 클라이언트로 수신될 때까지의 시간을 측정해보자.

이를 위해 RTT를 알아야 하는데,

`RTT(round-trip-time)`란 **작은 패킷이 클라이언트로부터 서버까지 가고, 다시 클라이언트로 되돌아오는 데 걸리는 시간**이다.

RTT는 패킷 전파 지연, 큐잉 지연, 처리 지연 등을 포함한다. (1장에 논의되어 있다.)

!https://user-images.githubusercontent.com/76640167/210491569-3638ca03-2d17-4eea-8a5f-d984bb831d00.png

사용자가 하이퍼링크를 클릭하면, 브라우저와 웹 서버 사이에서 TCP 연결을 시도한다. 이는 `3-way handshake`를 포함한다.

> 즉, 클라이언트가 서버로 작은 TCP 메시지를 보내고, 서버는 작은 메시지로 응답하고, 마지막으로 클라이언트가 다시 서버에 응답한다.
> 

서버가 작은 메시지로 응답하면 한 RTT가 계산된다. 이때, **클라이언트는 HTTP 요청 메시지를 TCP 연결로 보내면서 세 번째 응답 부분을 함께 보낸다.**

일단, 요청 메시지가 도착하면 서버는 HTML 파일을 TCP 연결로 보내고 이 요청은 또 하나의 RTT를 필요로 한다.

> 즉, 대략 총 응답 시간은 2RTT와 HTML 파일을 서버가 전송하는 데 걸리는 시간을 더한 것이다.
> 

**비지속 연결의 단점**

1. **각 요청 객체에 대한 새로운 연결이 설정되고 유지되어야 한다.**
    - TCP 버퍼가 할당되어야 하고, TCP 변수들이 클라이언트와 서버 양쪽에 유지되어야 하는데이는 수많은 클라이언트들의 요청을 동시에 서비스하는 웹 서버에는 심각한 부담이다.
2. 매번 2RTT를 필요로 한다.

**지속(persistent) 연결**

HTTP/1.1 지속 연결에서 서버는 응답을 보낸 후에 TCP 연결을 그대로 유지한다. (비지속 연결도 지원한다.)

**같은 클라이언트와 서버 간의 이후 요청과 응답은 같은 연결을 통해 보내진다.**

즉, 같은 서버에 있는 여러 웹 페이지들을 하나의 지속 TCP 연결을 통해 보낼 수 있다.

이들 객체에 대한 요구는 진행 중인 요구에 대한 응답을 기다리지 않고 연속해서 만들 수 있다. (파이프라이닝, pipelining)

일반적으로 HTTP 서버는 일정 기간 사용되지 않으면 연결을 닫는다.

HTTP의 디폴트 모드는 파이프라이닝을 이용한 지속 연결을 사용한다.

**기존의 HTTP/1.1**

지속적인 연결을 사용할 때 **웹 페이지당 오직 하나의 TCP 연결**을 가짐으로써,

아래 설명하듯이 서버에서의 소켓 수를 줄이며 전송되는 각 웹 페이지는 공정한 네트워크 대역폭을 가질 수 있다.

그러나 하나의 TCP 상에서 모든 웹페이지를 보내면 `HOL(Head of Line) 블로킹 문제`가 발생할 수 있다.

**HOL 블로킹 문제**

비디오 아래 수많은 작은 객체들을 포함할 때 서버와 클라이언트 사이에 저속에서 중간 속도의 **병목 링크**가 있다고 하자.

비디오 클립은 병목 링크를 통과하는데 오래 걸리는 반면, **작은 객체들은 비디오 클립 뒤에서 기다림이 길어진다.**

즉, 비디오 클립이 객체들을 블로킹하게 된다.

HTTP/1.1에서는 **여러 개의 병렬 TCP 연결**을 열어서 위 문제를 해결해왔다.

**TCP 혼잡 제어**

TCP 혼잡 제어는 각 TCP 연결이 공정하게 병목 링크를 공유하여 **같은 크기의 가용한 대역폭을 공평하게 나누게 해준다.**

만일 n개의 TCP 연결이 병목 링크에서 작동하고 있다면, 각 연결은 대략 대역폭의 1/n 씩을 사용하게 된다.

하나의 웹 페이지를 전송하기 위해 여러 개의 병렬 TCP 연결을 열게 함으로써 브라우저는 일종의 속임수로 링크 대역폭의 많은 부분을 받게 된다.

많은 HTTP/1.1 브라우저들은 6개까지 병렬 TCP 연결을 열고 HOL을 막을 뿐만 아니라 더 많은 대역폭을 사용할 수 있게 한다.

**HTTP/2 프레이밍(framing)**

> HTTP/2의 주요 목표 중 하나는 하나의 웹 페이지를 전송하기 위한 병렬 TCP 연결의 수를 줄이거나 제거하는 데 있다.
> 

이는 서버에서 열고 유지되는 데 필요한 소켓의 수를 줄일 뿐만 아니라 목표한 대로 TCP 혼잡 제어를 제어할 수 있게 하는 데 있다.

그러나 웹 페이지를 전송하기 위해 오직 하나의 TCP 연결만을 사용하게 될 경우에 HTTP/2는 HOL 블로킹을 피하기 위해 신중하게 구현된 메커니즘이 필요하다.

> 💡 HTTP/2 프레이밍(framing)이란, HTTP 메시지를 독립된 프레임들로 쪼개고, 인터리빙(interleaving)하고, 반대편 사이트에서 재조립하는 것이다.
> 

예를 들어, 비디오 클립과 크기가 작은 객체 8개의 요청이 들어오면, 서버는 9개의 객체를 보내기 위한 TCP 병렬 요청을 받게된다.

이때 (1) 비디오 클립을 1000개의 프레임으로 나누고 (2) 각 객체를 2개의 프레임으로 나누어 비디오 클립으로부터 하나의 프레임을 전송한다.

이후 프레임 인터리빙을 이용하여 각 소형 객체의 첫 번째 프레임을 보내고 이를 반복하여 HOL 블로킹을 피할 수 있다.

**HTTP 메시지를 독립된 프레임들로 쪼개고 인터리빙하고 반대편 사이트에서 재조립하는 것이야말로 HTTP/2의 가장 중요한 개선점이다.**

프레이밍은 HTTP/2 프로토콜의 프레임으로 구현된 다른 프레이밍 서브 계층에 의해 이루어진다.

서버가 HTTP 응답을 보내고자 할 때, 응답은 프레이밍 서브 계층에 의해 처리되며 프레임들로 나눠진다.

응답의 헤더필드는 하나의 프레임이 되고, 메시지 본문은 하나의 프레임으로 쪼개진다.

응답 프레임들은 서버의 프레이밍 서브 계층에 의해 인터리빙된 후 하나의 지속적인 TCP 연결상에서 전송된다.

프레임들이 클라이언트에 도착하면 프레이밍 서브 계층에서 처음 응답메시지로 재조립되며 브라우저에 의해 처리된다. (클라이언트에서 서버로 요청할 때도 마찬가지이다.)

각 HTTP 메시지를 독립적인 프레임으로 쪼개는 것 외에도 프레이밍 서브 계층은 프레임을 바이너리 인코딩한다.

바이너리 프로토콜은 파싱하기에 효율적이고, 더 작은 프레임 크기를 갖고, 에러에 강하다.

**메시지 우선순위화**

`메시지 우선순위화`는 개발자들로 하여금 요청들의 상대적 우선 순위를 조정할 수 있게 함으로써 애플리케이션의 성능을 최적화할 수 있게 해준다.

클라이언트가 하나의 특정 서버로 동시에 여러 개의 요청을 할 때, 각 메시지에 1에서 256 사이의 가중치를 부여함으로써 요청에 우선순위를 매길 수 있다.

> 높은 수치일수록 높은 우선순위를 갖는다.
> 

서버는 가장 높은 우선순위의 요청을 위한 프레임을 제일 먼저 보낼 수 있다.

클라이언트 또한 각 의존도에 따라 메시지의 ID를 지정하여 서로 다른 메시지들 간의 의존성을 나타낼 수 있다.

**서버 푸싱**

HTTP/2의 또 다른 특징은 서버로 하여금 **특정 클라이언트 요청에 대해 여러 개의 응답을 보낼 수 있게 해주는 데 있다.**

처음 요청에 대한 응답 외에도, 서버는 클라이언트의 요청 없이도 추가적인 객체를 클라이언트에게 **푸시**하여 보낼 수 있다.

이는 HTML 기반 페이지가 웹 페이지를 완벽하게 구동시킬 필요가 있는 객체들을 가리킬 수 있기에 가능하다.

이러한 객체에 대한 HTTP 요청을 기다리는 대신 서버는 HTML을 분석할 수 있고, 필요한 객체들을 식별할 수 있고,

**해당 객체들에 대한 요청이 도착하기도 전에** 해당 객체들을 클라이언트로 보낸다.

서버는 해당 요청들을 기다리는 데 소요되는 추가 지연을 없앤다.

### **2.7 소켓 프로그래밍: 네트워크 애플리케이션 생성**

네트워크 애플리케이션을 생성할 때는 두 프로그램, 클라이언트와 서버 프로그램을 작성해야 한다.

두 프로그램을 실행하면 프로세스가 생성되고, 두 프로세스가 소켓으로부터 읽고 쓰기를 통해 서로 통신한다.

클라이언트 - 서버 애플리케이션에는 두 가지 형태가 있다.

> 1️⃣ HTTP 등의 RFC에 정의된 표준 프로토콜을 구현하는 클라이언트-서버 애플리케이션
> 

이 애플리케이션을 구현할 때 그 프로토콜과 연관된 port를 사용하여야 한다.

> 2️⃣ 개인의 독점적인 네트워크 애플리케이션으로 RFC 또는 다른 곳에 공식적으로 출판되지 않은 애플리케이션 계층 프로토콜을 채택하여 구현하는 애플리케이션
> 

다른 독립 개발자는 이 애플리케이션과 상호작용하는 코드를 개발할 수 없다. 이 애플리케이션을 구현할 때는 잘 알려진 포트번호를 사용하지 않도록 유의해야 한다.

개발자는 TCP, UDP 프로토콜 중 어떤 프로토콜을 사용해야 하는지 각 프로토콜의 특징을 고려하여 선택해야 한다.

**2.7.1 UDP 소켓 프로그래밍**

UDP를 사용할 때에는 송신 프로세스가 데이터의 패킷을 소켓 문 밖으로 밀어내기 전에, 먼저 패킷에 목적지 주소를 붙여넣어야 한다.

이 패킷이 송신자의 소켓을 통과한 후 인터넷은 이 목적지 주소를 이용하여 그 패킷을 인터넷을 통해 수신 프로세스에 있는 소켓으로 라우트(route)할 것이다.

패킷이 수신 소켓에 도착하면 수신 프로세스는 소켓을 통해 그 패킷을 추출하고, 다음에 패킷의 콘텐츠를 조사하여 적절한 동작을 취한다.

**목적지 주소**

패킷에 목적지 주소를 포함함으로써 인터넷의 라우터는 목적지 호스트로 인터넷을 통해 패킷을 라우트할 수 있다.

호스트는 각자 `IP 주소`를 식별자로 갖는다.

그러나 호스트는 하나 혹은 그 이상의 소켓을 갖는 많은 네트워크 애플리케이션 프로세스를 수행하고 있을 수 있기 때문에,

**목적지 호스트 내의 특정한 소켓을 식별할 필요가 있다.**

소켓이 생성될 때 `포트 번호`라고 하는 프로세스 식별자가 소켓에 할당된다.

즉, 패킷에는 IP 주소와 포트번호로 구성된 목적지 주소가 붙게 되며, 송신자의 출발지 주소(IP와 port)도 붙게 된다.

일반적으로 이렇게 주소를 붙이는 것은 하부 운영체제가 자동으로 실행한다.

**소켓 프로그래밍**


위 그림은 UDP 서비스 상에서 통신하는 클라이언트와 서버의 주요 소켓 관련 활동을 나타낸다.

1. 클라이언트는 키보드로부터 한 줄의 문자를 읽고 그 데이터를 서버로 보낸다.
2. 서버는 그 데이터를 수신하고 문자를 대문자로 변환한다.
3. 서버는 수정된 데이터를 클라이언트에게 보낸다.
4. 클라이언트는 수정된 데이터를 수신하고 그 줄을 화면에 나타낸다.

위 순서로 작동하는 간단한 클라이언트-서버 애플리케이션을 만들 예정이다.

**UDPClient.py**

소켓을 생성할 때는 따로 소켓의 포트 번호를 명시하지 않아도 되며, 운영체제가 이 작업을 대신 수행한다.

```
# socket module이다. 이 module을 통해 소켓을 생성할 수 있다.
from socket import *

#서버의 IP 혹은 서버의 호스트 이름을 할당한다.
serverName = ’hostname’

# 목적지 port 번호를 나타낸다.
serverPort = 12000

# 클라이언트 소켓을 생성한다. AF_INET은 IPv4를 사용하고 있음을 나타내고, SOCK_DGRAM은 UDP 소켓임을 의미한다.
clientSocket = socket(AF_INET, SOCK_DGRAM)

# 보낼 메시지를 입력 받는다.
message = Input(’Input lowercase sentence:’)

# 소켓으로 바이트 형태를 보내기 위해 먼저 encode()를 통해 바이트 타입으로 변환한다.
# sendTo() 메서드는 목적지 주소를 메시지에 붙이고 그 패킷을 프로세스 소켓인 clientSocket으로 보낸다.
# 클라이언트 주소도 같이 보내지는데 이는 자동으로 수행된다.
clientSocket.sendto(message.encode(),(serverName, serverPort))

# 패킷 데이터는 modifiedMessage에 저장되고, 패킷의 출발지 주소(IP, port)는 serverAddress에 할당된다.
# recvfrom() 메서드는 2048의 버퍼 크기로 받아들인다.
modifiedMessage, serverAddress = clientSocket.recvfrom(2048)

# 출력
print(modifiedMessage.decode())

# 소켓 닫기
clientSocket.close()

```

**UDPServer.py**

while 문을 통하여 한 번의 통신 이후에도, 계속 다음 UDP 패킷이 도착하기를 기다린다.

```
from socket import *

# 포트 번호
serverPort = 12000

# UDP 소켓 생성
serverSocket = socket(AF_INET, SOCK_DGRAM)

# 12000 포트 번호를 소켓에 할당한다. 이를 통해 서버 IP 주소의 12000 포트로 패킷을 보내면 해당 소켓으로 패킷이 전달된다.
serverSocket.bind((’’, serverPort))

print(”The server is ready to receive”)

while True:
    # 패킷이 서버에 도착하면 데이터는 메세지에 할당되고 패킷의 출발지 주소는 clientAddress에 저장된다.
    # 해당 주소로 서버는 응답을 어디에 보내야할지 알 수 있다.
    message, clientAddress = serverSocket.recvfrom(2048)

    # 바이트 데이터를 decode()하고 대문자로 변환한다.
    modifiedMessage = message.decode().upper()

    # 클라이언트 주소를 대문자로 변환된 메시지에 붙이고, 그 결과로 만들어진 패킷을 서버에 보낸다.
    # 서버의 주소도 같이 보내지는데 이는 자동으로 수행된다.
    serverSocket.sendto(modifiedMessage.encode(), clientAddress)

```

**2.7.2 TCP 소켓 프로그래밍**

`TCP`는 **연결 지향 프로토콜**로, 서로 데이터를 보내기 전에 먼저 TCP 연결을 설정할 필요가 있다.

TCP 연결을 생성할 때 클라이언트 소켓 주소와 서버 소켓 주소를 연결과 연관시킨다.

연결이 설정된 후 소켓을 통해 데이터를 TCP 연결로 보내면 된다.

**환영 소켓과 연결 소켓**

서버 프로세스가 실행되면 클라이언트 프로세스는 서버로의 TCP 연결을 시도하는데, 이는 **클라이언트 프로그램에서 TCP 소켓을 생성**함으로써 가능하다.

TCP 소켓을 생성할 때, 서버에 있는 `환영(welcome) 소켓`의 주소(IP, port #)를 명시한다.

소켓을 생성한 후 클라이언트는 3-way handshake를 하고 서버와 TCP 연결을 설정한다. (핸드셰이킹은 프로그램에서 전혀 인지 못한다.)

핸드셰이킹 동안 서버는 **해당 클라이언트에게 지정되는 새로운 소켓**을 생성한다. 이를 `연결 소켓`이라고 한다.

애플리케이션 관점에서 볼 때 클라이언트의 소켓과 서버의 연결 소켓은 파이프에 의해 직접 연결된다.

파이프를 통해 클라이언트는 자신의 소켓으로 임의의 바이트를 보낼 수 있으며, 서버 프로세스가 그것을 수신하는 것을 TCP가 보장한다. 이는 서버 입장에서도 마찬가지이다.

**소켓 프로그래밍**

위 그림은 전형적인 클라이언트-서버 TCP 연결 구조이다.

UDP 프로그램과 똑같은 기능을 하는 프로그램을 작성해보자.

**TCPClient.py**

주석은 UDP와 다른 부분을 위주로 작성하였다.

```
from socket import *

serverName = ’servername’
serverPort = 12000

# 클라이언트 소켓을 의미한다. SOCK_STREAM으로 TCP 소켓임을 명시했다.
# UDP 때와 마찬가지로 따로 출발지 주소를 명시하지 않는다. (운영체제가 대신 해준다.)
clientSocket = socket(AF_INET, SOCK_STREAM)

# 클라이언트가 TCP 소켓을 이용하여 서버로 데이터를 보내기 전에 TCP 연결이 먼저 클라이언트와 서버 사이에 설정되어야 한다.
# 해당 라인으로 TCP 연결을 시작하고, connect() 메서드의 파라미터는 연결의 서버 쪽 주소이다.
# 이 라인이 수행된 후에 3-way handshake가 수행되고 클라이언트와 서버 간에 TCP 연결이 설정된다.
clientSocket.connect((serverName, serverPort))

sentence = raw_input(’Input lowercase sentence:’)

# 클라이언트 소켓을 통해 TCP 연결로 보낸다. UDP 소켓처럼 패킷을 명시적으로 생성하지 않으며 패킷에 목적지 주소를 붙이지 않는다.
# 대신 클라이언트 프로그램은 단순히 문자열에 있는 바이트를 TCP 연결에 제공한다.
clientSocket.send(sentence.encode())

# 서버로부터 바이트를 수신하기를 기다린다.
modifiedSentence = clientSocket.recv(1024)
print(’From Server: ’, modifiedSentence.decode())

# 연결을 닫는다. 이는 클라이언트 TCP가 서버의 TCP에게 TCP 메시지를 보내게 한다.
clientSocket.close()

```

**TCPServer.py**

```jsx
from socket import *

serverPort = 12000

# TCP 소켓 생성
serverSocket = socket(AF_INET, SOCK_STREAM)

# 서버의 포트 번호를 소켓과 연관시킨다.
serverSocket.bind((’’, serverPort))

# 연관시킨 소켓은 대기하며 클라이언트가 문을 두드리기를 기다린다.
# 큐잉되는 연결의 최대 수를 나타낸다.
serverSocket.listen(1)
print(’The server is ready to receive’)

while True:
    # 클라이언트가 TCP 연결 요청을 하면 accept() 메소드를 시작해서 클라이언트를 위한 연결 소켓을 서버에 생성한다.
    # 그 뒤 클라이언트와 서버는 핸드셰이킹을 완료해서 클라이언트의 소켓과 연결 소켓 사이의 TCP 연결을 생성한다.
    connectionSocket, addr = serverSocket.accept()
    sentence = connectionSocket.recv(1024).decode()
    capitalizedSentence = sentence.upper()
    connectionSocket.send(capitalizedSentence.encode())
    
    # 응답을 보내고 연결 소켓을 닫는다. 그러나 환영소켓인 serverSocket이 열려있어 다른 클라이언트가 서버에 연결을 요청할 수 있다. 
    connectionSocket.close()
```

**3.2 다중화와 역다중화**

> 💡 트랜스포트 계층 다중화(multiplexing)와 역다중화(demultiplexing)
> 
> 
> 네트워크 계층이 제공하는 `호스트 대 호스트 전달 서비스`에서
> 
> 호스트에서 동작하는 애플리케이션에 대한 `프로세스 대 프로세스 전달 서비스`로 확장하는 과정
> 
1. 목적지 호스트에서의 트랜스포트 계층은 바로 아래의 네트워크 계층으로부터 `세그먼트`를 수신한다.
    
    > 트랜스포트 계층은 호스트에서 동작하는 해당 애플리케이션 프로세스에게 이 세그먼트의 데이터를 전달하는 의무를 가진다.
    > 
2. 트랜스포트 계층은 세그먼트(데이터)를 중간 매개자인 `소켓(socket)`에게 전달한다.
    - 프로세스는 네트워크 애플리케이션의 한 부분으로서 소켓을 가지고 있다.
    - 이는 네트워크에서 프로세스로, 한 프로세스로부터 네트워크로 **데이터를 전달하는 출입구 역할**을 한다.
    - 각각의 소켓은 하나의 유일한 식별자, `포트 번호(port number)`를 가진다.

---

```
Q. 수신한 트랜스포트 계층 세그먼트는 어떻게 적절한 소켓으로 향하는가?

```

각각의 트랜스포트 계층 세그먼트는 세그먼트에 **필드 집합**을 가지고 있으며,

트랜스포트 계층은 수신 소켓을 식별하기 위해 이러한 필드를 검사한 후 해당 소켓으로 보낸다.

**✅ 역다중화(demultiplexing)**

트랜스포트 계층 세그먼트의 데이터를 **올바른 소켓으로 전달하는** 작업을 말한다.

**✅ 다중화(multiplexing)**

1. 출발지 호스트에서 소켓으로 부터 데이터를 모으고,
2. 이에 대한 세그먼트를 생성하기 위해 각 데이터에 헤더 정보로 캡슐화(encapsulation) 한다.
3. 그 세그먼트들을 **네트워크 계층으로 전달한다.**

**트랜스포트 계층 다중화의 두 가지 요구사항**

1. 소켓은 **유일한 식별자**를 갖는다. (= 포트 번호)
2. 각 세그먼트는 세그먼트가 **전달될 적절한 소켓을 가리키는 특별한 필드**를 갖는다.
    - `출발지 포트 번호 필드(source port number field)`
    - `목적지 포트 번호 필드(destination port number field)`
    

**역다중화 서비스의 순서**

1. 호스트의 각 소켓은 포트 번호를 할당받는다.
2. 세그먼트가 호스트에 도착하면,
    1. 트랜스포트 계층은 세그먼트 안의 **목적지 포트 번호를 검사**하고,
    2. 그에 상응하는 소켓으로 세그먼트를 보낸다.
3. 세그먼트의 데이터는 소켓을 통해 해당되는 프로세스로 전달된다.

이는 UDP의 기본적인 동작 방식과 같다.

**비연결형 다중화와 역다중화**

> 💡 UDP 소켓은 목적지 IP 주소와 목적지 포트 번호로 구성된 두 요소로 된 집합에 의해 식별된다.
> 

따라서 만약 2개의 UDP 세그먼트가 **같은 목적지 IP 주소와 목적지 포트 번호**를 가진다면,

이 2개의 세그먼트는 같은 목적지 소켓을 통해 같은 프로세스로 향할 것이다.

그렇다면 **출발지 포트 번호**는 무슨 목적으로 사용되는가?

> 출발지 포트 번호는 ‘회신 주소’의 한 부분으로 사용된다.
> 

아래 그림처럼, B가 A에게로 세그먼트를 보내기를 원할 때

B에서 A로 가는 **세그먼트의 목적지 포트 번호는** A로부터 B로 가는 세그먼트의 **출발지 포트 번호로부터 가져온다.**

**연결지향형 다중화와 역다중화**

**TCP 소켓**

> 💡 TCP 소켓은 4개 요소의 집합(four-tuple)에 의해 식별된다.
> 
- 출발지 IP 주소
- 출발지 포트 번호
- 목적지 IP 주소
- 목적지 포트 번호

특히, **다른 출발지 IP 주소 또는 다른 출발지 포트 번호**를 가지고 도착하는 2개의 TCP 세그먼트는 2개의 다른 소켓으로 향하게 된다.

*(초기 연결 설정 요청을 전달하는 TCP는 제외)*

**TCP 연결 설정**

1. TCP 서버 애플리케이션은 `환영(welcome) 소켓`을 갖고 있다.
    
    이 소켓은 `포트 번호 12000`을 가진 **TCP 클라이언트로부터 연결 설정 요청을 기다린다.** *(아래 그림 참고)*
    
2. TCP 클라이언트는 **소켓을 생성하고,** `연결 설정 요청 세그먼트`**를 보낸다.**
    - 연결 설정 요청은 `목적지 포트 번호 12000`과 TCP 헤더에 설정된 특별한 연결 설정 비트(3.5절에서 설명)를 가진 TCP 세그먼트를 통해 보내진다.
    - 이 세그먼트는 `출발지 포트 번호`를 포함하는데, 이것은 클라이언트가 선택한 번호이다.
3. 서버 프로세스로 동작하는 컴퓨터의 호스트 운영체제가 `목적지 포트 12000`을 포함하는 연결 요청 세그먼트를 수신하면,
    
    이 세그먼트를 **포트 번호 12000으로 연결 수락을 기다리는 서버 프로세스로 보낸다.**
    
4. 서버는 연결 요청 세그먼트의 4개 요소의 집합에 주목한다.
    
    > 서버 호스트는 동시에 존재하는 많은 TCP 소켓을 지원할 수 있다.
    > 
    - **새롭게 생성된 연결 소켓은 4개 요소의 집합의 네 가지 값에 의해 식별된다.**
    - 따라서 그다음에 도착하는 세그먼트의 출발지 포트, 출발지 IP 주소, 목적지 포트, 목적지 IP 주소가 **전부 일치하면,**그 세그먼트는 이 소켓으로 역다중화될 것이다.

**웹 서버와 TCP**

> 💡 서버는 각기 다른 클라이언트가 보낸 세그먼트를 출발지 IP 주소와 출발지 포트 번호로 구별한다.
> 

같은 웹 서버 애플리케이션과 통신하기 위해 **같은 목적지 포트 번호(80)를 이용하는 두 클라이언트**에 대한 예시를 보자.

- **호스트 C가 서버 B로** 2개의 HTTP 세션을 시작
- **호스트 A가 서버 B로** 하나의 HTTP 세션을 시작

호스트 A, 호스트 C, 서버 B는 **각자 유일한 IP 주소인 A, C, B를 각각 가지고 있다.**

- 호스트 C는 2개의 출발지 포트 번호(26145, 7532)를 자신의 HTTP 연결에 할당한다.
- 호스트 A는 호스트 C와 독립적으로 출발지 포트 번호를 선택하므로, **이것 또한 HTTP 연결에 출발지 포트로 26145를 할당할 수 있다.**

> 이렇게 하더라도, 2개의 연결은 다른 출발지 IP 주소를 가지기 때문에 서버 B는 여전히 올바르게 역다중화할 수 있다.
> 

---

웹 서버는 각각의 연결에 따라서 새로운 `프로세스`를 만든다.

이들 **프로세스는 각자** `연결 소켓`**을 가지며**, 이 연결 소켓을 통해 HTTP 요청을 수신하고, HTTP 응답을 전송한다.

> 그러나 연결 소켓과 프로세스 사이에 항상 일대일 대응이 이루어지는 것이 아니다.
> 
- 오늘날의 많은 고성능 웹 서버는 하나의 프로세스만 사용한다.
- 각각의 새로운 클라이언트 연결을 위해 새로운 연결 소켓과 함께 새로운 `스레드(thread)`를 생성한다.

**Persistent & Non-persistent HTTP**

**지속적인(persistent) HTTP**

지속적인 연결의 존속 기간에 클라이언트와 서버는 **같은 서버 소켓을 통해** HTTP 메시지를 교환한다.

**비지속적인(non-persistent) HTTP**

모든 요청/응답마다 새로운 TCP 연결이 생성되고 종료된다.
